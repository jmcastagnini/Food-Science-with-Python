{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7K6aU9YfDSbE"
   },
   "source": [
    "# Statistics in Food Science\n",
    "## Statistical Software\n",
    "- Open source / Free software:\n",
    "    - [R](https://www.r-project.org) and [R-Studio](https://rstudio.com)\n",
    "\n",
    "    - [PSPP](https://www.gnu.org/software/pspp/)\n",
    "\n",
    "- Commercial:\n",
    "    - [Minitab](https://www.minitab.com/en-us/)\n",
    "    - [Statgraphics](https://www.statgraphics.com)\n",
    "    - [SPSS](https://www.ibm.com/analytics/spss-statistics-software)\n",
    "    - [Statistica](https://onthehub.com/statistica/)\n",
    "\n",
    "The use of statistical and mathematical methods is usually divided into **univariate** (graph analysis, descriptive statistics), **bivariate** (correlation, linear regression analysis) and **multivariate** methods (exploratory, class-modeling, and classification methods ([Nunes et al., 2015](http://dx.doi.org/10.1016/j.foodres.2015.06.011))\n",
    "\n",
    "In food science and technology, statistics may be used for different purposes:\n",
    "\n",
    "- design of experiments\n",
    "- modeling of response variables using response surface methodology, and recently,\n",
    "- the application of multi-variate statistical methods has been more widely spread (Alezandro, Granato, Lajolo, & Genovese, 2011; Besten et al., 2013).\n",
    "\n",
    "In this sense, as mentioned by Granato, Calado, and Jarvis (2014), **descriptive analysis** (basic statistics: means, median, correlation, linear regression, standard deviation, among others) **followed by inferential statistics** (i.e., analysis of variances and multiple comparison ofmeans) **are the most frequently used methods**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0KpJYKCADfp6"
   },
   "source": [
    "## The development and reengineering of products and optimization of processes in food research\n",
    "\n",
    "There are two main options to conduct these studies:\n",
    "\n",
    "- using a design of an experiment (DOE) coupled to response surface methodology (Bas & Boyaci, 2007; Bassani, Nunes, & Granato, 2014; Domínguez- Perles, Teixeira, Rosa, & Barros, 2014; Farris & Piergiovanni, 2009; Granato, Castro, Ellendersen, & Masson, 2010; Granato, Grevink, Zielinski, Nunes, & van Ruth, 2014)\n",
    "\n",
    "     > DOE is also applied to study the effects of independent variables (factors) on selected responses (screening methods). The use of screening experimental designs, such as the $2^2$ and $2^3$ types, is useful because through this approach, factors that have less significant effects may be unconsidered in a future study (Silva, Sant'Ana, & Massaguer, 2010).\n",
    "     >\n",
    "     > In general, when DOE is used in combination with response surface methodology to analyze the experimental data, multiple regression equations relating to the independent variables with the response are obtained and one can use this mathematical equation to predict the quantitative value of the response (inside the range of the tested values).\n",
    "     >\n",
    "     > **For industrial purposes**, this methodology can be of great interest as the data can be used to have a quantitative idea about the impact of changing the factors on the response variables (Badoei-Dalfard & Karami, 2013; Joshi, Yadav, & Desai, 2008; Pedro, Granato, & Rosso, in press).\n",
    "\n",
    "- or by using the \"one variable at a time\" approach, that is, the application of random levels of selected factors (i.e. ingredients or process parameters) (Bassett et al., 2014; Boobier, Baker, & Davis, 2006; Granato, Castro, Piekarski, Benincá, & Masson, 2011; Haj-Isa & Carvalho, 2011).\n",
    "    > Disadvantages:\n",
    "    > * the main effects of factors and their interactions cannot be calculated\n",
    "    > * the relationship between the response and the factors cannot be estimated\n",
    "    > * is required a high number of experiments to conduct the research\n",
    "    > * the solution obtained usually does not represent the \"optimal\" conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5aAGu5PDiA2"
   },
   "source": [
    "## Statistical assumptions for the application of inferential methods\n",
    "\n",
    "### 1. Normality\n",
    "\n",
    "Testing whether a set of data (usually n ≥ 4 observations) follows a normal distribution (ND) is of major relevance for statistical procedures, namely parametric tests (such as analysis of variance (ANOVA), correlation analysis, simple and multiple regression and t-tests). In practice, there are two ways to check experimental results for conformance to a ND: graphically or by using numerical methods. The graphical method, usually displayed by normal quantile–quantile plots, histograms or box plots, is the simplest and easiest way to assess the normality of data; however, this method should not be used for small data sets due to lack of sufficient quantitative information (Razali & Wah, 2011). Numerical approaches are the best way to test for the normality of data, including determination of kurtosis and skewness; for example, tests such as those attributed to Anderson–Darling (AD), Kolmogorov–Smirnov (KS), Shapiro–Wilk (SW), Lilliefors (LF), andCramér vonMises (CM).\n",
    "\n",
    "A population ND can be described as a bell-shaped curve under which approximately 95% of values lie within the range mean (μ) ± 2 standard deviations (σ) and approximately 99% lie within the range μ ±3σ. The standard deviation is a measure of the dispersion of values around the mean value and is determined as the square root of the variance. The mean value ($\\overline{x}$) and standard deviation (s) of a set of data obtained by analysis of random samples provide estimates of the population statistics.\n",
    "\n",
    "Since, for a ND, approximately 95% of results would be expected to lie within the range $\\overline{x} \\pm 2 s$ we describe the lower and upper bounds of this range as the 95% Confidence Limits (CL) of the results; similarly, we describe the bounds of the 99% CL as $\\overline{x} \\pm 2 s$. What this means is that 19 of 20 results of an analysis would be expected to lie within the bounds of the 95% CL, but by definition one result might occur outside this limits; similarly, one in 100 results might be expected to lie outside the 99% CL bounds.\n",
    "Results that do fall outside the CLs are often referred to as **\"outliers\"** whether such results are true values or occur because of faults in analytical technique can never be known, but it is essential to assess the frequency with which outliers occur.\n",
    "Various techniques exist for estimating the likelihood of outlier results (assuming ND) of which the most useful are those described by Youden and Steiner (1975), Anonymous (1994) and Horwitz (1995).\n",
    "\n",
    "\n",
    "### 2. Homoscedasticity\n",
    "\n",
    "Is an important feature for parametric tests when conducting inferential tests (i.e. multiple comparison of means). Homoscedasticity of data means that different groups present similar standard deviations. This is important because otherwise the chances of getting false positives can be higher than the α-value established at the beginning of the experiment `(P < 0.05 or 5%)` (McDonald, 2014).\n",
    "It has been observed that some researchers pay no attention to the application of appropriate statistical techniques to validate experimental data. For all types of measurements, variance homogeneity, called homoscedasticity, should be assessed graphically and by a numerical test (Montgomery, 2009).\n",
    "\n",
    "Several tests are available to check the equality of variances on data from three or more samples and include those of Cochran, Bartlett, Brown–Forsythe and Levene; the F-test is used to check for homogeneity of two variances. Overall, the same test must be applied for all data sets in order to be coherent and consistent because these tests use different ways to calculate the statistics, and for one test the data can be considered homoscedastic and heteroscedastic for the other.\n",
    "\n",
    "Thus, the researcher has two distinct situations: the variances are essentially equal or they are not. If the test shows that variances are heterogeneous, two possibilities exist: to use a non-parametric test or to transform the dependent variable in order to obtain a constant variance of the residues. Different types of transformations can be used, such as the logarithmic (log10, natural log), Box–Cox, square root or inverse function transformations, depending on the distribution of the data. This approach can be used when the analytes do not follow a normal distribution (as in the case of microbiological and sensory data). Data transformationmay normalize the distribution, stabilize the variances or/and stabilize a trend (Rasmussen & Dunlap, 1991). Parametric analysis of transformed data provides a better strategy than non-parametric analysis because the former is more powerful and accurate than the latter (Gibbons, 1993). However, it is important to keep in mind that the point of the transformation is to ensure the validity of the analysis (ND, equal standard deviations) and not to ensure a certain type of result (Rasmussen & Dunlap, 1991). It is worth noting that transformation should be avoided if possible since the transformed variable loses its absolute identity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6SU5lRBDmFU"
   },
   "source": [
    "## Number of replicates\n",
    "\n",
    "Usually authors state the ‘analyses were performed in triplicate’, but sometimes, this information is not totally clear: independent samples means the process of making beer, for example, must be performed three times to obtain three products. If the scientist produces only one beer and the analysis of ethanol is performed three times, this situation is not an example of true/genuine replicates. The ideal situation is that the beer is produced three times and each beer must be analyzed in triplicate for a certain attribute.\n",
    "\n",
    "The reason to perform the analysis in triplicate is based on the fact that measuring three times the same property is an acceptable standard concerning the precision and the work to be performed. According to Passari et al. (2011), the mean value of triplicate measurements is the best estimate of the analyte in the sample, and the standard deviation is the best estimate of the experimental error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "REII8uCHDtkx"
   },
   "source": [
    "## Experimental data type\n",
    "\n",
    "Experimental data can assume various forms: the distribution of data values for a measured variable following replicate analysis of samples may be either **continuous**, i.e. it can assume any value within a given range, or **discrete**, i.e. it can assume only whole number (integer) values. The latter generally applies to counts rather than measurements as in qualitative microbiological tests. In some situations, experimental variables may be **nominal**, e.g.male or female gender selection for taste trials, **categorical**, e.g. values can be sorted according to defined categories such as good, average or poor for qualitative taste tests, or **ordinal**, e.g.values are ranked on a semi-quantitative basis using a predetermined scale such as hedonic taste panel scores. It is essential to understand data set designations in order to carry out appropriate analyses. Special nonparametric procedures are required for analysis of nominal, categorical and ordinal data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tLs6xRVDxeh"
   },
   "source": [
    "## Parametric statistics in Food Science\n",
    "\n",
    "Depending on the statistical distribution of data, sample size, and\n",
    "homoscedasticity, samples and treatments can be compared using parametric or non-parametric tests. Parametric tests should be used when data are normally distributed and there is homogeneity of variances, as shown by the Shapiro–Wilk and Levene (or F) tests, respectively.\n",
    "Then, a Student's t-test is used to check for differences between two mean values or an ANOVA is used when three or more mean values need to be compared.\n",
    "\n",
    "[](statistical_steps.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_3POZXCpPW12"
   },
   "source": [
    "<img src=\"https://drive.google.com/uc?id=1Zp71ygUDEt40muCep6rbsC9iNrRD46wv\" width=\"640\" height=\"480\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDGtMJNNEpZ_"
   },
   "source": [
    "## Comparing two samples\n",
    "\n",
    "When the mean values for a specific characteristic in two data sets are to be compared and both data sets are normally distributed and have similar variance, a Student's t-test should be used. The null hypothesis ($H_0$) is that the mean values do not differ; the alternative ($H_a$) is that they do differ.\n",
    "\n",
    "Samples are considered to be independent when they differ in nature and do not depend on one another.\n",
    "\n",
    "A paired sample t-test is used if each of several samples are analyzed in parallel for the same characteristic by two different methods or if tests.\n",
    "\n",
    "If the variances are not strictly equal, a correction factor (Welch's test) should be included in the statistical analysis. If the data do not conform to ND, non-parametric tests should be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQkTVMftHL0V"
   },
   "source": [
    "## Analysis of variances for three or more data sets\n",
    "\n",
    "Analysis of variances (ANOVA) is a parametric statistical tool that partitions the observed variance into components that arise from different sources of variation. In its simplest form, ANOVA provides a statistical test of whether or not the means of several groups are all equal. In this sense, the null hypothesis, $H_0$, says there are no differences among results from different treatments or sample sets; the alternative hypothesis ($H_a$) is that the results do differ.\n",
    "\n",
    "Three alternative models can be used in an ANOVA: fixed effects, random effects or mixed effects models. The fixed effects model is appropriate when the levels of the independent variables (factors) are\n",
    "set by the experimental design. The random effects model, which is often of greatest interest to a researcher, assumes that the levels of the effects are randomly selected from an infinite population of possible levels (Calado & Montgomery, 2003).\n",
    "\n",
    "Depending on the number of factors to be analyzed, we can have:\n",
    "\n",
    "1. A one-way ANOVA in which only one factor is assessed. This is the case for relatively simple comparisons of physicochemical, colorimetric, chemical and microbiological analytes (Alezandro, Granato, Lajolo, & Genovese, 2011; Corry, Jarvis, Passmore, & Hedges, 2007; Granato & Masson, 2010; Oroian, 2012). Another important application of one-way ANOVA is when different groups of test animals that are treated with an extract/drug and compared to a control group (Macedo et al., 2013).\n",
    "\n",
    "2. A 2-way ANOVA is used for two factors in which only the main effects are analyzed. The 2-way ANOVA determines the differences and possible interactions when response variables are from two or more categories. The use of 2-way ANOVA enables comparison and contrast of variables resulting from independent or joint actions (MacFarland, 2012). This type of ANOVA can be employed in sensory evaluation when both panelists and samples are sources of variation\n",
    "(Granato, Ribeiro, & Masson, 2012) or when the consistency of the panelists needs to be assessed;\n",
    "\n",
    "3. A factorial ANOVA for n factors, that analyzes the main and the interaction effects is the most usual approach for many experiments, such as in a descriptive sensory or microbiological evaluation of foods and beverages (Ellendersen, Granato, Guergoletto, & Wosiacki, 2012; Jarvis, 2008; Mon & Li-Chan, 2007);\n",
    "\n",
    "4. A repeated-measures (RM) ANOVA is used to analyze designs in which responses on two or more dependent variables correspond to measurements at different levels of one or more varying conditions. Benincá, Granato, Castro, Masson, and Wiecheteck (2011) used a RM-ANOVA to examine results from assessments of different instrumental color attributes for a mixture of juices from yacón (Peruvian ground apple) tubers and yellow passion fruit as a function of storage time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_J1RfoAMecQ"
   },
   "source": [
    "## Post-hoc tests to compare three or more samples/treatments\n",
    "\n",
    "In practice, the choice of the best test to compare mean values depends on the investigator's experience. We recommend the use of Duncan's multiple range test (MRT) or Fisher's Least Significant Difference (LSD) test because of their high power to detect significant\n",
    "differences in mean values or Tukey's Honest Significant Difference (HSD) test.\n",
    "\n",
    "The Tukey HSD is a single-step multiple comparison generally used in conjunction with an ANOVA to identify if one mean value differs significantly from another. It compares all possible pairs of means and is the most useful test for multiple comparisons. However, the method is not statistically robust, being sensitive to the requirement that the\n",
    "means need to follow a normal distribution. Even when there is a significant difference between a pair of means, this test often does not pin-point it (Calado & Montgomery, 2003). However, many researchers\n",
    "claim that the Tukey test is the procedure of choice since it avoids Type II errors.\n",
    "Fisher's LSD test is a statistical significance test used where sample sizes are small and when the distribution of the residues is normal (p Levene ≥ 0.05). The test is much more robust than Tukey but\n",
    "it is the most sensitive to Type I errors; yet it provides an important tool for comparing means after an ANOVA procedure (Carmer & Swanson, 1973). Duncan's MRT is not restricted to data conforming strictly to ND and does not require a significant overall ‘between-treatments’ F test but it is also likely to give Type I errors. The Tukey–Kramer HSD single-step multiple comparison procedure compares mean values using a ‘Studentized Range’, which is more conservative than the original Tukey HSD test, and can be used with unequal group sizes. It is much stricter than many other tests but is less likely to give Type I errors (Keppel & Wickens, 2004).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eo_z8aXTWNjK"
   },
   "source": [
    "## Non-parametric statistics in Food Science\n",
    "Non-parametric procedures use ranked data rather than actual data values. The data are ranked from the lowest to the highest and each value is assigned, in order, the integer values from 1 to n (where n = total sample size) (Hollander & Wolfe, 1973). Non-parametric methods provide an objective approach when there is no reliable (universally recognized) underlying scale for the original data and when there is concern that the results of standard parametric techniques would be criticized for their dependence on an artificial metric (Siegel, 1956)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HsOCuiYMYMDT"
   },
   "source": [
    "## Bivariate correlation analysis\n",
    "\n",
    "Correlation is a method of analysis used to study the possible association between two continuous variables. The correlation coefficient (r) is a measure that shows the degree of association between both variables (Granato, Calado, Oliveira, & Ares, 2013). This parametric test requires both data sets to consist of independent random variables that\n",
    "conform to ND. The correlation coefficient measures the degree of linear association between the two sets of data (A and B), and its value lies between −1 and +1. The closer the absolute value, |r|, is to 1, the stronger the correlation between the data values (Ellison, Barwick, & Farrant, 2009).\n",
    "\n",
    "Many workers calculate the Pearson linear correlation coefficient in order to seek to determine the strength of association between data sets. However, when more than five variables are analyzed, the analysis is compromised because correlation coefficients do not assess simultaneous association among results for all variables, which makes it diffi-\n",
    "cult to understand and interpret the structure and patterns of the data. For example, if one considers five sets of response variables (A, B, C, D and E), it is necessary to calculate the correlation coefficients, and their significance, for each data set pair.\n",
    "It is easy to understand and interpret up to three correlations coefficients but, in order to better understand multiple responses, a more sophisticated multivariate statistical approach, such as principal component analysis, clustering techniques, linear discriminant analysis should be used (Besten et al., 2012, 2013; Granato et al., in press; Zielinski et al., in press).\n",
    "\n",
    "When large sets of results (≥30) are analyzed, data should be formally checked for normality. If the data do not follow a normal distribution a non-parametric approach, such as the Spearman's rank correlation\n",
    "coefficient, should be used to analyze for any correlation between the responses. Fig. 9 shows the steps to follow when two data sets (each with n ≥ 8) are to be analyzed with respect to correlation. Spearman's correlation coefficient (ρ) should be used when either or both data sets do not conform to ND, when the sample size is small, or when the variables are measured as ordinals i.e. first, fifth, eighth, etc. in a sequence of values. The Spearman correlation coefficient does not require the assumption that the relationship between variables is linear.\n",
    "\n",
    "There is no scientific consensus about the qualitative assessment of correlation coefficients, that is, whether a correlation coefficient is truly strong, moderate or weak. Granato, Castro, Ellendersen, and Masson (2010) established an arbitrary scale for the strength of correlations between variables using the following criteria: perfect (|r| = 1.0), strong (0.80 ≤ |r| b 1.0), moderate (0.50 ≤ |r| b 0.80), weak (0.10 ≤ |r| b 0.50), and very weak (almost none) correlation (0.10 ≤ |r|).\n",
    "\n",
    "There are two major concerns regarding correlation tests: the significance of the correlation and the interpretation of results. Firstly, to assume a statistically significant association between variables the p-value of the correlation coefficient should be b0.05. Granato, Katayama, and Castro (2011) showed that with large data sets, the correlation coefficient is often statistically significant even at a moderate or low r value. On the other hand, when the data set is small (Granato, Freitas, & Masson, 2010), high values of r may be observed but the statistical probability of the correlation is not significant (p N 0.05). Secondly, when a correlation coefficient is calculated, it is not always possible to assume causation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIWR-7a0eaYW"
   },
   "source": [
    "## Regression analysis\n",
    "\n",
    "Regression analysis is used to examine the relationship between sets of dependent and independent variables and includes many techniques for modeling and analyzing two or more sets of data.\n",
    "\n",
    "In its simplest form, regression is used to assess the relative effects under defined process conditions. It may be used also to establish calibration curves for chemical, physical and biological assays for continuous data sets. Calibration curves require at least five concentration levels including a blank value with adequate (at least triplicate) replication of tests at each concentration level.\n",
    "\n",
    "\n",
    "In order to perform a regression analysis it is essential to:\n",
    "\n",
    "- Test data for the presence of outliers (at 95 or 99% of confidence) using the Grubb's test for each concentration level;\n",
    "- Ensure the homogeneity of variances in the concentration levels of the calibration curve by using one the tests described above\n",
    "(Section 2.1.2).\n",
    "- Build the model (i.e. the graph) to display the analyte concentration versus the response (absorbance, area, etc.);\n",
    "- Test the significance of the regression and its lack of fit through the F-test and a one-factor ANOVA. Provided that the response is directly and linearly correlated to the concentrations then the\n",
    "regression coefficient should be significant. Evidence for lack of fit (p b 0.05), may be due to a non-linear response, to excessive variation in the replicates at one or more of the test values or\n",
    "the use of an over-extended independent variable range. In this case, removing the highest values and repeating the statistical analysis should reduce the range of concentrations. Evidence for lack of linearity may indicate that a nonlinear model (quadratic, for example) might be more appropriate for the method, and therefore, alternative models should be evaluated.\n",
    "- Determine the following statistical parameters by means of the regression analysis:\n",
    "    - The regression equation (y = ax + b), where y is the dependent estimate at independent concentration level (x), a is the slope of the line and b is the linear intercept when x = 0;\n",
    "    - The standard deviation of the estimated parameters and model;\n",
    "    - The statistical significance of the estimated parameters;\n",
    "    - The coefficient of determination ($R^2$ ; regression coefficient) and the adjusted $R^2$.\n",
    "    \n",
    "The regression model is considered suitable to the experimental data when:\n",
    "1. The standard deviation of the parameter is at least 10% lower than the corresponding parameter value;\n",
    "2. The standard deviation of the proposed mathematical model is small;\n",
    "3. The parameters of a model are statistically significant otherwise they will not contribute to the model.\n",
    "    >It is a myth to consider that if $R^2$ > 90% the model is excellent (Montgomery & Runger, 2011). This is only one criterion to evaluate the goodness of fit of the model. If $R^2$ is low (< 70%), the mathematical\n",
    "model is not good; on the other hand, if $R^2$ is high (> 90%), it means that you should continue the analysis and check the other criteria. It is\n",
    "noteworthy that in some applications, depending on the type of analysis, e.g. evaluation of sensory data, the coefficient of determination may be considered good if $R^2$ > 60%;\n",
    "\n",
    "4. The statistical significance, obtained from the F-test of an ANOVA analysis of the proposed mathematical model is at least p < 0.05;\n",
    "5. Analysis of the residuals (experimental value for a response variable minus value predicted by the mathematical model) must conform to ND and have a constant variance, as described above. This is a necessary condition for the application of some\n",
    "post-hoc tests such as t and F.\n",
    "\n",
    "> It is important to recognize that the regression and correlation coefficients describe different parameters. Regression describes the goodness\n",
    "of fit of a model; correlation estimates the linear relationship of two variables.\n",
    "\n",
    "A common mistake is to use $R^2$ to compare models. $R^2$ is always higher if we increase the order of a model (linear in comparison to quadratic, for example). For example, a third order polynomial has a higher $R^2$ than a second order polynomial because there are more terms, but it does not necessarily mean that the first is the better model. An analysis of the degrees of freedom (number of experimental points minus number of parameters from the model) needs to be carried out. A model with more terms requires estimation of more coefficients so fewer degrees of freedom remain. Thus, another criterion needs to be used: the adjusted regression coefficient — $R^2_{adj}$. This coefficient adjusts for the number of explanatory terms in a model relative to the number of\n",
    "data points and its value is usually less than or equal to that of $R^2$.\n",
    "When comparing models, the one with the highest adjusted coefficient is the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbuQoGnO6i2V"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "StatFoodScience.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
